{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger and pub-annotation generator for COVID-19\n",
    "This is a project in the course EDAN70, Projects in Computer Science, at LTH Sweden. The goal of this project is using NLP for tagging words in articles and generate a collection of pub-annotations in order to aid the scientific community in finding relevant research for their field.\n",
    "\n",
    "## Authors\n",
    "Annie Tallind, Lund University, Faculty of Engineering <br/>\n",
    "Kaggle ID: [atllnd](https://www.kaggle.com/atllnd/) <br/>\n",
    "Github ID: [annietllnd](https://github.com/annietllnd/) <br/>\n",
    "\n",
    "Sofi Flink, Lund University, Faculty of Engineering <br/>\n",
    "Kaggle ID: [sofiflinck](https://www.kaggle.com/sofiflinck/) <br/>\n",
    "Github ID: [obakanue](https://github.com/obakanue) <br/>\n",
    "\n",
    "## Credit\n",
    "Dictionaries were generated using golden- and silver-standard implemented by Aitslab.\n",
    "\n",
    "## About project\n",
    "The scientific field is overwhelmed by the constant influx of new research being published. During the crisis of the COVID-19 pandemic it is even more crucuial to developing tools for aiding researchers in finding relevant data. For this purpose a tagger was implemented for generating pub-annotations in order to give reserachers a tool to summerize articles. Hopefully this tool will also be good groundwork for filtering articles in other fields as well.\n",
    "\n",
    "## The source code\n",
    "This is constructed in a way to guide the reader through the code and the different elements of the tagger. It goes through each step, from loading data in to the implementation to tagging, pub-annotation generation and precision and recall evaluation.\n",
    "\n",
    "### TODO list\n",
    "\n",
    "### Load and process datafiles\n",
    "**Load JSON-file paths** <br/>\n",
    "We import 'os' in order to use the method 'listdir()' which returns a list containing the names of the entries in the directory given by path. In this case tdatafilesdatafileshe path to the JSON-file articles in directory 'comm_use_subset_100' which is saved in constant 'DIRECTORY_NAME'. They will be loaded in to the array 'article_paths'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIRECTORY_NAME = 'comm_use_subset_100'\n",
    "article_paths = os.listdir(DIRECTORY_NAME)\n",
    "article_dicts_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load JSON-files** <br/>\n",
    "The import 'json' is used in order to be able to process the contents of each file to a dictionary. These dictionaries are added to the list 'article_dicts_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for article_name in article_paths:\n",
    "    if article_name == \".DS_Store\":  # For MacOS users skip .DS_Store-file\n",
    "        continue                     # generated.\n",
    "    full_path = DIRECTORY_NAME + '/' + article_name\n",
    "    with open(full_path) as article:\n",
    "        article_dicts_list.append(json.load(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load CSV-file with metadata** <br/>\n",
    "We also need the metadata which contains complementing information for the JSON-articles. First we need ti import pandas, this is a library for data analysis with built in methods to handle CSV files. The method 'load_metadata()' reads a CSV file and creates a dataframe for the content. Next a list witch each articles metadata as a dictionary is created, 'metadata_list'.   \n",
    "Since we want to be able to easily access the position for a particular element in the list, the indices where mapped as values with 'sha' id as key. This way we can easily find and access any element in the list, even if it has two different id's for the same articles which sometimes occurs. The list with metadata and the dictionary mapping a 'sha' id to an index is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Returns list with metadata and dictionary with sha as keys and indices of metadata list as values.\n",
    "    \"\"\"\n",
    "    metadata_csv_path = 'metadata_comm_use_subset_100.csv'\n",
    "    metadata_frame = pd.read_csv(metadata_csv_path,\n",
    "                                 na_filter=False,\n",
    "                                 engine='python')\n",
    "    metadata_list = metadata_frame.to_dict('records')\n",
    "    index = 0\n",
    "    metadata_indices_dict = dict()\n",
    "    for data in metadata_list:\n",
    "        shas = data['sha'].split('; ', 1)\n",
    "        for sha in shas:\n",
    "            metadata_indices_dict.update({sha: index})\n",
    "        index += 1\n",
    "    return metadata_list, metadata_indices_dict\n",
    "\n",
    "metadata_list, metadata_indices_dict = load_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabularies and create patterns used for tagging\n",
    "**Load vocabularies** <br/>\n",
    "Vocabularies are text files with words we wish to tagg in the articles. The ones used here where generated using Aitslabs golden and silver standard called 'Virys_SARS-CoV-2' and 'Disease_COVID-19'. We want to make an easy and streamlined process where we simply can add as many vocabularies as necessary. We create a dictionary 'VOCABS_COL_DICT' with the class name as key and the list of words as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABS_COL_DICT = {'Virus_SARS-CoV-2':\n",
    "                   [row.strip() for row in\n",
    "                   open('Supplemental_file1.txt')],\n",
    "                   'Disease_COVID-19':\n",
    "                   [row.strip() for row in\n",
    "                   open('Supplemental_file2.txt')]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary with patterns** <br/>\n",
    "In addition to vocabularies we wish to handle certain word patterns, these get their own class. For now we only have one pattern: for all words ending in 'vir' and it's class name is 'chemical_antiviral'. Here, similiar to 'VOCABS_COL_DICT', 'PATTERNS_DICT' will use the class name as a key, and this time the pattern is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Patterns:\n",
    "1. All words ending in 'vir' case insensitive in class 'chemical_antiviral'.\n",
    "\"\"\"\n",
    "PATTERNS_DICT = {'chemical_antiviral':\n",
    "            r'(?i)\\b\\S*vir\\b'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag articles and generate pub-annotations\n",
    "**Define method for processing an article** <br/>\n",
    "So before constructing a loop for iterating through all articles and processing them we define a function for how an article should be processed. For now, an article and corresponding metadata are sent in as input arguments for the function as dictionaires. We only need some of the information from the metadata and as such a helper function 'obtain_metadata_args extracts 'cord_uid', 'source_x' and 'pmcid' from the dictionary to a list which is returned and saved in 'metadata_info'. The sections we want to filter through are 'metadata' (which contains 'title'), 'abstract' and 'body_text'.   \n",
    "Now we will find each section of the article by looping through our 'sections' list. If the section is 'metadata' we wish to retrieve the 'title' from the articled dictionary. In order to be able to process all sections equally in the code all of the sections creates a new list 'section_paragraphs' with the help of list comprehension. This means that this is done for the article section 'title' as well even if articles never have more than one title. Since we are only interested in the 'title' in 'metadata' section, we wish to update the 'section' string to 'title' instead. For the other sections there is a possibility of several text paragraphs for one section. With list comprehension all the paragraphs are saved in 'section_paragraphs' list. If the paragraphs in a section is empty we handle that by assigning the list as an list with an empty string. The counter 'paragraph_index' keeps track of the amout of paragraphs in a section and resets for every new section.   \n",
    "The tagging will be done for each paragraph, this way we can process more demands for the filtering and tagging of articles before exporting the pub-annotations to a file. The tagging of paragraphs are handled with 'tag_paragraph' which we will walk through a bit later. For constructing the complete denotation method get_paragraph_denotation() is used with the 'url' section in corresponding metadata dictionary is used as input argument. If the denotation is empty, that is '\\[ \\]', we will not create a pub-annotations since no matches where found. The function 'construct_pubannotation()', returns the full pub-annotations string while 'export_pubannotation()' exports the string to a json-file. For every section, 'file_index' will increment with every section, so for every article, at most 3 files with pub-annotations will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_article(article_dict, metadata_dict):\n",
    "    \"\"\"\n",
    "    Process article for each section and paragraph and generate pub-annotations for export to file.\n",
    "    \"\"\"\n",
    "    file_index = 0\n",
    "    metadata_info = obtain_metadata_args(metadata_dict)\n",
    "    sections = ['metadata', 'abstract', 'body_text']\n",
    "    for section in sections:\n",
    "        if section == 'metadata':\n",
    "            section_paragraphs = [article_dict[section]['title']]\n",
    "            section = 'title'\n",
    "        else:\n",
    "            section_paragraphs = [section['text'] for section in\n",
    "                                  art() icle_dict[section]]\n",
    "        if not bool(section_paragraphs):\n",
    "            section_paragraphs = ['']\n",
    "        paragraph_index = 0\n",
    "        for paragraph in section_paragraphs:\n",
    "            tag_paragraph(paragraph)\n",
    "            denotation = get_paragraph_denotation(metadata_dict['url'])\n",
    "            if not re.fullmatch(r'\\[\\]', denotation):\n",
    "                annotation = construct_pubannotation(metadata_info,\n",
    "                                                      paragraph_index,\n",
    "                                                      paragraph,\n",
    "                                                      denotation)\n",
    "                export_pubannotation(metadata_info[0],\n",
    "                                      file_index,\n",
    "                                      section,\n",
    "                                      annotation)\n",
    "            paragraph_index += 1\n",
    "        file_index += 1  # Increment with each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_article(): obtain_metadata_args()** <br/>\n",
    "As mentioned above we will now walk through the helper functions called in 'tag_article()'. The first function used is 'obtain_metadata_args' which uses the metadata dictionary as argument input. The function simply extracts as mentioned earlier the sections of interest in the dictionary and returns them as elements in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_metadata_args(metadata_dict):\n",
    "    \"\"\"\n",
    "    Returns necessary columns from metadata dictionary. Index 0 gives cord_uid, index 1 gives source_x, index 2 gives\n",
    "    pmcid.\n",
    "    \"\"\"\n",
    "    cord_uid = metadata_dict['cord_uid']\n",
    "    source_x = metadata_dict['source_x']\n",
    "    pmcid = metadata_dict['pmcid']\n",
    "    metadata_info = [cord_uid, source_x, pmcid]\n",
    "    return metadata_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_article(): tag_paragraph()** <br/>\n",
    "The function takes the paragraph string as input argument, for every new paragraph we wish to clear the dictionary 'paragraph_matches'. The dictionary contains match objects as key and word class as value, this dictionary is nesessaru in order to keep track on what words are to be tagged with whith which word class. We will walk through the priority rules further down.   \n",
    "The function will iterate through every dictionary and pattern in 'VOCABS_COL_DICT' and 'PATTERNS_DICT' and tag for every occurance in the paragraph string. This is done in helper function 'tag_pattern()'. We create regular expression patterns for every word in a dictionary, since both pattern matching and word matching uses similar code we can avoid repeating ourselves by using the function 'tag_pattern()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_matches = dict()\n",
    "def tag_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    For a paragraph, iterate through all vocabularies and patterns and tag using corresponding regex-pattern.\n",
    "    \"\"\"\n",
    "    paragraph_matches.clear()\n",
    "    for vocabulary in VOCABS_COL_DICT:\n",
    "        for word in VOCABS_COL_DICT[vocabulary]:\n",
    "            pattern = fr'(?i)\\b{word}(es|s)?\\b'\n",
    "            tag_pattern(pattern, paragraph, vocabulary)\n",
    "\n",
    "    for word_class in PATTERNS_DICT:\n",
    "        tag_pattern(PATTERNS_DICT[word_class], paragraph, word_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_paragraph(): tag_pattern()** <br/>\n",
    "We need to import 're' in order to use the methods for searching with regular expressions. We will use 'finditer()' which will return match objects of all found matches for a certian pattern when iterating through a string. In our case 'text' will be our paragraphs. The input arguments for our helper functions are the regex pattern, string of text we wish to tag, and what word class the pattern belonged to. For all matches we can find we will send them to helper function 'is_match_priority()' which will return true if the match is prioritized and is to be added to our dictionary with match objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tag_pattern(pattern, text, word_class):\n",
    "    \"\"\"\n",
    "    For a particular pattern, find matches in paragraph and add to 'paragraph_matches' dictionary, if match is\n",
    "    prioritized.\n",
    "    \"\"\"\n",
    "    for match in re.finditer(pattern, text):\n",
    "        is_priority = is_match_priority(pattern, match.group(0), word_class)\n",
    "        if is_priority:\n",
    "            paragraph_matches.update({match: word_class})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_pattern(): is_match_priority()** <br/>\n",
    "For now we don't have a lot of rules since we only use three different word classes. For 'Virus_Sars-CoV-2' and 'Disease_COVID-19' only the longest match should be tagged, if there exists an older shorter match it will be deleted from the dictionary 'paragraph_matches' and true is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match_priority(pattern, new_word_match, word_class):\n",
    "    \"\"\"\n",
    "    Checks priorites of tagging for vocabularies. For 'Virus_SARS-CoV-2' and 'Disease_COVID-19' if already pattern\n",
    "    matches with existing match in 'paragraph_matches' then only the longest match will be kept in the dictionary.\n",
    "    Returns 'True' if new match is to be added (prioritized).\n",
    "    \"\"\"\n",
    "    for match in paragraph_matches:\n",
    "        word_match = match.group(0)\n",
    "        if word_class == 'Virus_SARS-CoV-2' or word_class == 'Disease_COVID-19':\n",
    "            prev_tagged = re.match(pattern, word_match)\n",
    "            if prev_tagged:\n",
    "                longest_match = max(new_word_match, word_match, key=len)\n",
    "                if longest_match == new_word_match:\n",
    "                    del paragraph_matches[match]\n",
    "                    return True\n",
    "                return False\n",
    "            return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_article(): get_paragraph_denotation()** <br/>\n",
    "The helper function uses the input argument a string representing the url for a particular article. For every match we have in 'paragraph_matches' we wish to create a denotation which is appended to 'denotations' list. The denotation for every match is generated with helper function 'construct_denotations()' which returns a string. Then all denotations are concatenated with helper function 'concat_denotations()' which concatenates the strings properly accordingly to pub-annotation format. The complete string is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_denotation(url):\n",
    "    \"\"\"\n",
    "    Constructs complete string denotation for a paragraph.\n",
    "    \"\"\"\n",
    "    denotations = []\n",
    "    for match in paragraph_matches:\n",
    "        denotations.append(construct_denotation(paragraph_matches[match],\n",
    "                                                str(match.start()),\n",
    "                                                str(match.end()), url))\n",
    "    return concat_denotations(denotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for get_paragraph_denotation(): construct_denotation()** <br/>\n",
    "The function takes input arguments 'idd', which is the word class string, string format of the begining of a match in a paragraph, string format of the end of the same match in a paragraph, and the url for a particular article retrieved from the metadata file. Here strings are properly concatenated to create a single match for a paragraph and then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_denotation(idd, begin, end, url):\n",
    "    \"\"\"\n",
    "    Returns a string denotation for a single match.\n",
    "    \"\"\"\n",
    "    idd = \"\\\"id\\\":\\\"\" + idd + \"\\\", \"\n",
    "\n",
    "    span = \"\\\"span\\\":{\\\"begin\\\":\" + begin + \",\" + \"\\\"end\\\":\" + end + \"}, \"\n",
    "\n",
    "    obj = \"\\\"obj\\\":\\\"\" + url + \"\\\"\"\n",
    "    denotation = \"{\" + idd + span + obj + \"}\"\n",
    "    return denotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for get_paragraph_denotation(): concat_denotation()** <br/>\n",
    "The argument for the helper function is the list with string elements where every element is a denotation in pub-annotation format for a single match. We now wish to merge the strings if there are several matches in a single paragraph. This string is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_denotations(denotations):\n",
    "    \"\"\"\n",
    "    Returns a complete denotation string of all separate denotations in\n",
    "    list parameter, or an empty string if there where no elements in the\n",
    "    list.\n",
    "    \"\"\"\n",
    "    if not bool(denotations):\n",
    "        return \"[]\"\n",
    "\n",
    "    full_denotation = ''\n",
    "\n",
    "    for denotation in denotations:\n",
    "        if denotation == denotations[-1]:\n",
    "            full_denotation += denotation\n",
    "        else:\n",
    "            full_denotation += denotation + \", \"\n",
    "    return \"[\" + full_denotation + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for tag_article(): construct_pubannotation()** <br/>\n",
    "This helper function creates the complete string for the pub-annotation to be exported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pubannotation(metadata_info, paragraph_index, text, denotation):\n",
    "    \"\"\"\n",
    "    Returns a string in pub-annotation format.\n",
    "    \"\"\"\n",
    "    cord_uid = \"\\\"cord_uid\\\":\\\"\" + metadata_info[0] + \"\\\", \"\n",
    "\n",
    "    source_x = \"\\\"sourcedb\\\":\\\"\" + metadata_info[1] + \"\\\", \"\n",
    "\n",
    "    pmcid = \"\\\"sourceid\\\":\\\"\" + metadata_info[2] + \"\\\", \"\n",
    "\n",
    "    divid = \"\\\"divid\\\":\" + str(paragraph_index) + \", \"\n",
    "\n",
    "    text = \"\\\"text\\\":\\\"\" + text + \"\\\", \"\n",
    "\n",
    "    project = \"\\\"project\\\":\\\"cdlai_CORD-19\\\", \"\n",
    "\n",
    "    denotations_str = \"\\\"denotations\\\":\" + denotation\n",
    "\n",
    "    return \"{\" + cord_uid + source_x + pmcid + divid + text + project + \\\n",
    "           denotations_str + \"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_pubannotation(idd, file_index, section, annotation):\n",
    "    \"\"\"\n",
    "    Export pub-annotation string to corresponding section file.\n",
    "    \"\"\"\n",
    "    file_name = idd + \"-\" + str(file_index) + \"-\" + section\n",
    "    text_file = open(\"out/\" + file_name + \".json\", \"wt\")\n",
    "    text_file.write(annotation)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(62, 77), match='new coronavirus'>\n",
      "<_sre.SRE_Match object; span=(777, 788), match='alisporivir'>\n",
      "<_sre.SRE_Match object; span=(928, 947), match='ritonavir/lopinavir'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e013b9608d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     article_name.replace('.json', '')]\n\u001b[1;32m      4\u001b[0m     \u001b[0mmetadata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprocess_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-99ce369641f6>\u001b[0m in \u001b[0;36mprocess_article\u001b[0;34m(article_dict, metadata_dict)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mparagraph_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msection_paragraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtag_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mdenotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_paragraph_denotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             annotation = construct_pubannotation(metadata_info,\n",
      "\u001b[0;32m<ipython-input-43-5af0c03cd523>\u001b[0m in \u001b[0;36mtag_paragraph\u001b[0;34m(paragraph)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mVOCABS_COL_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mfr'(?i)\\b{word}(es|s)?\\b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtag_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPATTERNS_DICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-468e66f9a761>\u001b[0m in \u001b[0;36mtag_pattern\u001b[0;34m(pattern, text, word_class)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprioritized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mis_priority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_match_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_priority\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36mfinditer\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 416\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"|)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mbreak\u001b[0m \u001b[0;31m# end of subpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0msourceget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for article_dict in article_dicts_list:\n",
    "    metadata_index = metadata_indices_dict[\n",
    "    article_name.replace('.json', '')]\n",
    "    metadata_dict = metadata_list[metadata_index]\n",
    "    process_article(article_dict, metadata_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
